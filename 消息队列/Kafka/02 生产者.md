# kafka 生产者

## 1. 发送流程

在消息发送的过程中，涉及到了==两个线程——main线程和 Sender 线程==。在 main 线程中创建了一个==双端队列 RecordAccumulatoor==。main 线程将消息发送给 RecordAccumulator，Sender线程不断从 RecodAccumulator 中拉取消息发送到 Kafka Broker。

![image-20220303092458348](https://lixianghong.oss-cn-beijing.aliyuncs.com/typore/image-20220303092458348.png)

- `batch.size:` 只有数据积累到 batch.size 之后，sender 才会发送数据。默认是 16K

- `linger.ms:` 如果数据迟迟未达到batch.size，sender 等待 linger.ms 设置的时间到了之后就会发送数据。单位 ms，默认是 0 ms，表示没有延迟

应答 acks 的级别

**0：**生产者发送过来的数据，不需要等数据落盘应答

**1：**生产者发送过来的数据，Leader 收到数据后应答

**-1（all）：**生产者发送过来的数据，Leader 和 ISR 队列里面的所有节点收齐数据后应答。-1 和 all 等价



## 2. 异步发送

### 2.1 普通异步发送

1）需求：创建 Kafka 生产者，采用异步的方式发送到 Kafka Broker

**创建 maven 工程**

1. 引入依赖

```xml
<dependencies>
    <dependency>
        <groupId>org.apache.kafka</groupId>
        <artifactId>kafka-clients</artifactId>
        <version>3.0.0</version>
    </dependency>
</dependencies>
```

2. 代码实现

```java
public class CustomProducer {
  public static void main(String[] args) {
    // 0 配置
    Properties properties = new Properties();
    // 连接集群 bootstrap.servers
    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "hadoop102:9092");
    // 指定对应的 key 和 value 的序列化类型 key.serializer
    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
    // 1 创建 Kafka 生产者对象
    KafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(properties);
    // 2 发送数据
    for (int i = 0; i < 5; i++) {
      kafkaProducer.send(new ProducerRecord<>("first", "atguigu"), new Callback() {
        @Override
        public void onCompletion(RecordMetadata metadata, Exception exception) {
          if (exception == null) {
            System.out.println("主题:" + metadata.topic() + "分区：" + metadata.partition());
          }
        }
      });
    }
    // 3 关闭
    kafkaProducer.close();
  }
}
```



## 3 分区策略

<img src="https://lixianghong.oss-cn-beijing.aliyuncs.com/typore/image-20220303110559893.png" alt="image-20220303110559893" style="zoom: 50%;" />

1. ==指明 partition 的情况下，直接将指明的值作为 partition 值==；例如 partition = ==0==，所有数据写入分区 ==0==

2. 没有指明 partition 值但有 key 的情况下，将 **key的hash值** 与 **topic的partition**数进行**取余**得到 partition 值

   例如：key1 的 hash 值 = ==5==，key2 的 hash值 = ==6==，topic 的partition 数 = ==2==，那么 key1 对应的 ==value1 写入 1号分区==，key2 对应的==value2 写入0号分区==

3. 既没有 partition 值有没有 key 值的情况下 Kafka 采用 Sticky Partition（粘性分区器），会随机选择一个分区，并尽可能一直使用该分区，待该分区的 ==batch已满或者已完成，kafka 再随机一个分区进行使用（和上一次的分区不同）==

## 4 自定义分区器

**1）需求**

例如：我们实现一个分区器实现，发送过来的数据如果包含 atguigu，就发往 0 号分区，不包含 atguigu，就发往 1 号分区。

**2）实现步骤**

（1）定义类实现 Partitioner 类

（2）重写 partition() 方法

```java
public class MyPartitioner implements Partitioner {
  @Override
  public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
    // 获取数据 atguigu hello
    String msgValues = value.toString();
    int partition;
    partition = msgValues.contains("atguigu") ? 0 : 1;
    return partition;
  }

  @Override
  public void close() {}

  @Override
  public void configure(Map<String, ?> configs) {}
}
```

## 5 数据可靠性

**kafka 应答 acks**

==0：==生产者发送过来的数据，不需要等待数据落盘应答

==1：==生产者发送过来的数据，Leader 收到数据后应答

==-1（all）：==生产者发送过来的数据，Leader+ 和 isr 队列里面的所有节点收齐数据后应答。-1 和 all 等价

![image-20220614160743400](https://lixianghong.oss-cn-beijing.aliyuncs.com/typore/image-20220614160743400.png)

![image-20220614161105884](https://lixianghong.oss-cn-beijing.aliyuncs.com/typore/image-20220614161105884.png)

**可靠性总结：**

acks = 1，生产者发送过来数据就不管了，可靠性差，效率高；

acks = 1，生产者发送过来数据 Leader 应答，可靠性中等，效率中等

acks = -1，生产者发送过来数据 Leader 和 ISR 队列里面所有 Follower 应答，可靠性高，效率低

在生产环境中，acks = 0使用很少，acks = 1一般用于传输普通日志，运行丢失个别数据；acks = -1一般用于传输和钱相关的数据，对可靠性要求比较高的场景

![image-20220614161430039](https://lixianghong.oss-cn-beijing.aliyuncs.com/typore/image-20220614161430039.png)



## 6 数据重复

- 至少一次 = ==ACK级别设置为 -1 + 分区副本大于等于 2 + ISR 里应答的最小副本数量大于等于 2==

- 最多一次 = ==ACK级别设置为0==

- 总结：
  - 至少一次 可以保证数据不丢失，但是==不能保证数据不重复==
  - 最多一次 可以保证数据不重复，但是==不能保证数据不丢失==
- 精确一次：对于一些非常重要的信息，如果和钱相关，要求数据==即不能重复也不丢失==



### 6.1 幂等性

**幂等性**指 Producer 不论向 Broker 发送多少次重复数据，Broker 端只会持久化一条，保证了不重复。

精确一次 = 幂等性 + 至少一次

**重复数据的判断标准：**具有<PID, Partition, SeqNumber> 相同主键的消息提交时，Broker 只会持久化一条。其中 ==PID 是 Kafka 每次重启都会分配一个新的；Partition 表示分区号； Sequence Number 是单调自增的。==

所以幂等性==只能保证的是在单分区单分区单会话内不重复==

如何开启幂等性？

​	开启参数 `enable.idempotence` 默认为 true，false 关闭。



### 6.2 生产者事务

==说明：开启事务，必须开启幂等性==

==Producer 在使用事务功能钱，必须限自定义一个唯一的 transactional.id。==有了 transactional.id，即使客户端挂掉了，它重启后也能继续处理未完成的事务

![image-20220614164504813](https://lixianghong.oss-cn-beijing.aliyuncs.com/typore/image-20220614164504813.png)











































































































